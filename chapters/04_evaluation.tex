% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}

This section provides an evaluation of the designed artifact in two ways. In the previous section, seven requirements have been defined for the designed system. First, it will be discussed how the designed system fits in these requirements. Second, a qualitative evaluation of the system will be presented. For this part of the evaluation, one hour interviews with five different researchers with peer review experience were conducted. The \acrfull{TAM} is used as a framework to assess the system usage qualitatively. The responses by the interviewees are categorized under \acrshort{TAM} variables and interpreted to determine their intention to use of the system. According to the responses, a basic \acrlong{TAM} is proposed. Various insights gained during the interviews about the system and peer reviewing in general are also shared.

\section{Evaluation of Design Requirements}

Based on open science principles and the problems discussed, seven requirements were defined for the system:

\begin{itemize}
  \item RQ1: Verifiability 
  \item RQ2: Transparency 
  \item RQ3: Open Data 
  \item RQ4: Open Standards 
  \item RQ5: Direct Trust 
  \item RQ6: Selective Disclosure 
  \item RQ7: Compatibility 
\end{itemize}

\subsubsection{1. Verifiability}

Verifiability, in short, encompasses the ability to check if a peer review has really taken place and the presented information about the peer review is correct. The designed system is based on \acrlong{VC}, which makes use of the Linked-Data Proofs or \acrshort{JSON} Web Tokens that allows the verification of credentials using digital signatures. By resolving the \lstinline{issuer} (journal) identifier, their public keys can be retrieved and the credential can be verified. 

\subsubsection{2. Transparency}

The designed system improves upon the existing ones in numerous ways in terms of transparency. First of all, the verification of the peer reviews is transparent, that is anyone can take the existing data and verify that the peer review credential is actually issued by the given journal, and that it is authentic. Current showcasing platforms verify the reviews themselves and mark the reviews as verified without information on how this verification is done. Also, the Veriview platform can complete the verification steps and inform the users on its user interface that the review is verified. By open sourcing the code this process is also kept transparent.

\subsubsection{3. Open Data}

Even though the nature of blinded peer reviews prevents complete openness of the data, the system makes it possible to keep as much data of peer reviews as possible open without losing verifiability. Additionally, the data stored in Veriview is identical to what is publicly available. 

\subsubsection{4. Open Standards}

The system is based on open standards under development such as \acrlong{VC}, \acrlong{DID}, \acrshort{JSON-LD}. The platforms are open sourced and anyone can easily create a similar platform. This would avoid future vendor lock-in and further contributes the openness of the system.

\subsubsection{5. Direct Trust}

The peer review credentials are issued directly by journals and the verification is also done through journal keys. The showcasing system only serves as a platform to share and host the credentials, and requires minimal trust. 

\subsubsection{6. Selective Disclosure}

By using Linked-Data Proofs with BBS+ Signatures, it is possible to derive credentials with a subset of the attributes, and with a zero-knowledge proof that ensures the knowledge of the original credential and a valid signature. This enables review authors to only share the non-identifiable information of a review without losing verifiability.

\subsubsection{7. Compatibility}

The proposed system allows the extension of the proposed peer review vocabulary or the use of different vocabularies instead. This enables to accommodate the data model of the credentials to different peer review processes but also keeps the interoperability by having the vocabularies published. 

\section{Interviews}

For the qualitative evaluation of the work, 5 different researchers with peer review experience were interviewed: Interviewee 1, Kevin Wittek, Interviewee 3, Tiago Paixao, and Joao Oliviera. Interviewer 1 and Joao Oliviera were active Publons users and others have heard of it. The interviews took around 1 hour, were recorded and transcribed with the interviewees' consents. The interviews encompass first a general discussion around the interviewee's peer review experience and their perspectives, followed by an introduction to existing review showcasing platforms, in particular Publons, and their experience if they have any. Next, the conceptual design of the system is presented. The interviewees are then given a hands-on experience of the prototype and they are asked to complete the basic user flow of the application. Finally, their review of the system is solicited. 

The interviews were run as free dialogues and did not follow a strict structure besides the flow described above. However, the following or similar questions were asked to interviewees over the course of the interviews:

\begin{itemize}
    \item Could you please introduce yourself and detail your peer review experience?
    \item How would you describe the peer review culture in your field?
    \item What motivates you to do the peer review for a manuscript?
    \item Has there been a case where you presented your reviews in an academic resume, or that it would be beneficial if you could present it?
    \item Are you using review showcasing platforms? If yes, could you describe your experiences?
    \item What consequences may arise in the future that Publons is the only owner of the peer review data?
    \item Do you think the value added by Veriview exceeds the complexity it creates?
    \item What might be a reason to share or not to share your peer reviews on such a platform?
    \item Who might be interested in consuming the peer review data on Veriview?
\end{itemize}

A recurring theme across all interviewees was the lacking constructiveness and quality in some reviews and that some review authors don't seem to invest sufficient time to write reviews that will improve the manuscript. This seems to dishearten the researchers who try to write good reviews. Another discouraging experience mentioned by interviewees has been the cases where the interviewees see a paper they carefully reviewed with constructive feedback being published elsewhere without any of their input being taken into account. Nevertheless, all but one interviewees state reciprocity and a sense of duty as the main motivations to do peer reviews. The second common motivation stated has been the early access to research in their fields and having an idea what other researchers are working on. These are in line with the existing research and surveys \parencite{Publons.2018, Taylor&Francis.2015, Ware.2008, Squazzoni.2013}. 

Our interviewees all include their review or editorial work in their academic resumes, but usually this information is given a low priority without much detail and placed at the bottom of the resume. Because as Kevin Wittek stated: "there is not really a culture that the reviews become part of your CV or of your reputation". If this was the case, for instance reviewing in a high ranking journal would increase the reputation of scientists, then the reviewers would invest more resources to reviewing, he also explains. Tiago Paixao also affirms that these contributions are not deemed important. Interviewer 3 pointed out having this information on the resume demonstrates that you are connected with certain communities and having recently a best peer reviewer prize awarded, they became aware that it is nice to have this type of work published. Joao Oliviera brought the value of these records into question as it does not convey any information on the quality of the review and reminded that many reviewers don't really put much effort. Interviewee 1 mentioned that including this field shows that one engaged with the broader science, and it contributes to their periodical evaluations. But at the same time, institutions don't want researchers to spend much time doing peer review and their main focus is publications and grants a researcher has, and researchers' incentives are not aligned to do reviews but to publish papers and get grants, as Tiago Paixao mentioned. "The peer review is really, in objective terms, a waste of time. It's really an altruistic thing you do, I think", he said.

Even though most interviewees include peer reviews in their resume, some of them didn't think verification would be a necessity. "I'm sure people inflate their CV's" said Interviewee 1, "...but I don't think there's much incentive to lie with your peer reviews.". Interviewee 3 said they are not aware of anyone falsely claiming he/she reviewed for a journal, or if there's a motivation to do that. They also think Publons, despite being a trusted third-party, is not incentivized to publish inaccurate data as this is their core business and such an action would impair their reputation heavily. 

On the one hand, these statements show that researchers care for their peer review work and want to demonstrate their efforts but the larger scientific community doesn't seem to regard review service as a measure in practice. More visibility and availability of reviews would be a positive change for researchers in this sense. However, the perspectives on the unnecessariness of proofs, unimportance of reviews, and that no one would lie with their review work demonstrates the need for further validation of the significance of the problem.

When asked about Publons, our interviewees said they heard other people using it. The active users started using it after receiving an automatic invitation following a review they submitted, and they denote that they don't put much effort on their profiles, and actively add reviews. 

When questioning about the consequences of Publons being the only owner of the peer review data, we received contrasting answers. Our interviewees mostly expressed they didn't consider this before. 2 of the reviewers didn't see this as a problem, one even favored centralization that it brings more accountability and makes it easier to manage the data. Also, some interviewees don't consider that Publons has all the data but only the ones shared by the users, therefore that it does not strictly control the data and only collects a portion of it. However, concerns have been raised that if the platform becomes a de facto standard for review recognition, it could create a harder vendor lock-in, similar to what the scientific community experienced in the acqusition of Mendeley by Elsevier\footnote{https://www.mysciencework.com/omniscience/elsevier-takes-over-mendeley-and-you-what-do-you-think}. The fact that this data is controlled by a commercial entity is not necessarily bad, but something to be cautious of, especially that in the future this data can be monetized in various ways that won't be favored from an open science standpoint. We observe that for the interviewees with interest in open science practices and decentralization, this concern is expressly higher. Another decentralization aspect was brought up by Interviewee 3 that this may cause established reviewers to get more invitations and others less, and a more equal distribution of the review work is favored. Such inequalities are already apparent in peer review \parencite{Warne.2016, Publons.2018,Ware.2008, Hochberg.2009} and the use of this data for reviewer matching could exacerbate this. 

We also introduced our conceptual design to the interviewees and observed their understanding of the concept. A common confusion was around how the credentials are issued and how the verification works. However, without needing to explain the technical details, they were able to understand that the proof shows the authenticity and the integrity of the review, and that a credential with a subset of the attributes of the original credential can be created. All interviewees agree that the designed artifact as it is, is more complex than the existing platforms, but they could easily see that these complexities can be abstracted away from the user by a Researchgate plugin or similar (Interviewee 3) or automating the Veriview-journal interactions (Tiago Paixao). The Interviewee 1 and Joao Oliviera, both Publons users, emphasize the need of effortlessness on these steps. Since researchers, currently, don't put too much value on showing their reviews, these steps have to be as easy as possible. Kevin Wittek points out the typical learning curve in \acrshort{SSI} systems which includes making users aware that the credentials and keys are under their responsibility, and the cases of wallet loss and key management need to be considered. Even though in our system \acrshort{DID}s are not a requirement for authors, credential management can become a usability issue as stated. 

When asked who would become a review data consumer, journals and journal editors looking for reviewers were the common answers. Also, sometimes manuscript authors are asked to recommend reviewers and they can also benefit from the availability of this data, Kevin Wittek suggested. He also added researchers that need to regularly report their work can include their review efforts through systems like ours. Additionally, the Interviewee 3 stated such a platform can benefit researchers that would like to improve their writings and reviewing skills by providing access to good reviews. Institutions can also make use of this data to better asses the researchers and see their contributions to the larger community, Tiago Paixao suggested.

\section{Technology Acceptance Model}

The use of information technologies in workplaces and organizations has been increasing significantly, improving productivity across many aspects. Scientific publishing is no exception \parencite[132]{Ware.2015}. Understanding the adoption of new technologies, therefore, is a major research interest and an established field. The \acrlong{TAM} \parencite{Davis.1985, Davis.1989, Davis.1989b} became a widely used framework for explaining the user acceptance and its validity has been tested many times since over a quarter century \parencite{Marangunic.2015}. \acrshort{TAM} proposes the perceived usefulness and the perceived ease of use as the main determinants of the attitude toward using the technology which also influences the behavioral intention to use. \cite{Davis.1989b} also found out does not fully mediate perceived usefulness and the
perceived ease of use and it is removed from the model. The model is rather parsimonious in number of variables, and different extensions to the model have been proposed \parencite{Marangunic.2015}. 

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/TAM.png}
  \caption{\acrlong{TAM} \parencite{Davis.1989b} } \label{fig:tam}
\end{figure}

Based on our findings from our interviews we discuss the variables that would influence the perceived usefulness and the perceived ease of use of our designed system by the users and propose a technology acceptance model. As users we consider only review authors but there are many potential types users such as journals, universities, research institutes etc. 

\subsection{External Variables}

\subsubsection{Significance of Reviews}

A common statement during the interviews was the insignificance of reviews in assessments, tenure, and overall researcher reputation. This was also apparent in how researchers include their review work in their resumes, by only sparing little space for this information. The disparity of this perception in the scientific community with the peer review's importance in scientific knowledge generation has been discussed in previous sections. The disparity has caught the attention of the community \parencite{Verissimo.2013, Tennant.2020} and the need for more recognition to peer review is being voiced. Researchers expect their institutions to require and recognize peer review contributions \parencite{Publons.2018} and state they would spend more time on reviews if it was recognized \parencite{Warne.2016}.

We define the variable broadly as significance of reviews, as this includes both the inherent value researchers give to reviews, the degree of recognition from institutions, and the attitude of the scientific community towards review work of a researcher. More significance of peer review works of a researcher would increase the need for such peer review showcasing and verification systems. We therefore propose that the significance of reviews will have a positive effect on the perceived usefulness of the technology. Interestingly, we argue, this is a chicken and egg problem, where the under-utilization of review recognition platforms are due to peer review's insignificance, and peer review's under-recognition is a result of unavailability of ways to demonstrate peer review work. Put other way, there exist the potential of a virtuous feedback loop: more visibility to peer review may foster peer review's significance and more significance of the process would create more demand to use showcasing platforms.

\subsubsection{Open Science Commitment}

Open Science Commitment can be defined as the degree of support of a researcher for open science practices. The designed artifact aims to remove trusted third parties from the review verification process and promote open data and transparency. This aspect is also the main differentiating property of the designed system from the existing ones. Taking this into account and based on our observations during the interviews, we propose that open science commitment of a researcher will have a positive impact on the perceived usefulness of the system. This factor would also influence the preference of a user between existing showcasing systems and our designed system. 

\subsubsection{Active Use Requirement}

The majority of the interviewees expressed the need for automation of the user actions on the platform. Researchers don't spend much time on their peer reviews once they write and submit it. As Interviewee 1 said, reviewers just do the review, send it, then move on to other tasks. They don't think about that review anymore. Joao Oliviera also highlighted the importance of effortless usage of the system, saying he also was invited to try an application similar to Reserachgate but did not continue using it as he didn't want to get into the trouble. Either the value added by the platform should be evident to put time on platform's tasks or it should be as effortless as possible. Google Scholar, since it is the primary online profile and shows a scientist's publications and citation metrics, is checked regularly by Interviewee 1 but they almost never spend time on their Publons profile. Based on these observations we propose that the active use requirement has a negative effect on the perceived ease of use. The most important component of this would be adding the reviews to the user profile. If this step could be automated, the perceived ease of use would highly increase.


\subsubsection{Institutional Demand}

warne: would spend more time on review if the institution recognizes

\subsubsection{Review Resume Eagerness}











% The Interviewee 1 and Joao Oliviera, both Publons users, emphasize the need of effortlessness on these steps. Since researchers, currently, don't put too much value on showing their reviews, these steps have to be as easy as possible. We can interpret this as the  